{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541208a6",
   "metadata": {},
   "source": [
    "## 多任务模型: 模糊图片 → SR修复 → 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a4e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "from NetSet import SRNet, ClassifyNet\n",
    "from datasets import CIFAR10Dataset, Fuzz\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_dir = \"results/multi\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std  = (0.2023, 0.1994, 0.2010)\n",
    "normalize = transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "\n",
    "class MultiTaskNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sr = SRNet()\n",
    "        self.classifier = ClassifyNet()\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        sr_out = self.sr(x)\n",
    "        sr_norm = self.normalize(sr_out)\n",
    "        cls_out = self.classifier(sr_norm)\n",
    "        return sr_out, cls_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33dae2",
   "metadata": {},
   "source": [
    "## 加载预训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c98fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预训练权重加载完成\n"
     ]
    }
   ],
   "source": [
    "model = MultiTaskNet().to(device)\n",
    "\n",
    "model.sr.load_state_dict(\n",
    "    torch.load(\"results/task1/model_epoch_100.pth\", map_location=device))\n",
    "\n",
    "model.classifier.load_state_dict(\n",
    "    torch.load(\"results/task2/model_epoch_100.pth\", map_location=device))\n",
    "\n",
    "print(\"预训练权重加载完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b89dd5",
   "metadata": {},
   "source": [
    "## 联合微调训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9913ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [  1/50] SR: 0.04343 | CLS: 0.71679 | Train: 76.06% | Test: 76.60%\n",
      "Epoch [  2/50] SR: 0.04209 | CLS: 0.64683 | Train: 78.06% | Test: 78.00%\n",
      "Epoch [  3/50] SR: 0.04169 | CLS: 0.60678 | Train: 79.62% | Test: 78.49%\n",
      "Epoch [  4/50] SR: 0.04109 | CLS: 0.59395 | Train: 79.85% | Test: 79.12%\n",
      "Epoch [  5/50] SR: 0.04091 | CLS: 0.57188 | Train: 80.70% | Test: 78.10%\n",
      "Epoch [  6/50] SR: 0.04052 | CLS: 0.55962 | Train: 80.99% | Test: 79.30%\n",
      "Epoch [  7/50] SR: 0.04081 | CLS: 0.54611 | Train: 81.54% | Test: 78.84%\n",
      "Epoch [  8/50] SR: 0.04044 | CLS: 0.52826 | Train: 81.96% | Test: 79.02%\n",
      "Epoch [  9/50] SR: 0.04043 | CLS: 0.52224 | Train: 82.19% | Test: 78.94%\n",
      "Epoch [ 10/50] SR: 0.04038 | CLS: 0.51672 | Train: 82.30% | Test: 79.40%\n",
      "Epoch [ 11/50] SR: 0.03972 | CLS: 0.50392 | Train: 82.83% | Test: 79.67%\n",
      "Epoch [ 12/50] SR: 0.03971 | CLS: 0.49567 | Train: 83.25% | Test: 80.33%\n",
      "Epoch [ 13/50] SR: 0.04057 | CLS: 0.48474 | Train: 83.59% | Test: 80.12%\n",
      "Epoch [ 14/50] SR: 0.03991 | CLS: 0.48316 | Train: 83.49% | Test: 80.10%\n",
      "Epoch [ 15/50] SR: 0.03987 | CLS: 0.47235 | Train: 83.83% | Test: 80.04%\n",
      "Epoch [ 16/50] SR: 0.04014 | CLS: 0.46522 | Train: 84.23% | Test: 80.57%\n",
      "Epoch [ 17/50] SR: 0.03978 | CLS: 0.45964 | Train: 84.44% | Test: 80.50%\n",
      "Epoch [ 18/50] SR: 0.04017 | CLS: 0.45314 | Train: 84.71% | Test: 80.68%\n",
      "Epoch [ 19/50] SR: 0.03996 | CLS: 0.44299 | Train: 84.91% | Test: 80.48%\n",
      "Epoch [ 20/50] SR: 0.03965 | CLS: 0.43669 | Train: 85.18% | Test: 79.99%\n",
      "Epoch [ 21/50] SR: 0.04011 | CLS: 0.43485 | Train: 85.14% | Test: 80.61%\n",
      "Epoch [ 22/50] SR: 0.04005 | CLS: 0.42872 | Train: 85.46% | Test: 80.59%\n",
      "Epoch [ 23/50] SR: 0.04001 | CLS: 0.41763 | Train: 85.78% | Test: 80.69%\n",
      "Epoch [ 24/50] SR: 0.03980 | CLS: 0.42006 | Train: 85.53% | Test: 80.85%\n",
      "Epoch [ 25/50] SR: 0.03997 | CLS: 0.41124 | Train: 85.87% | Test: 80.65%\n",
      "Epoch [ 26/50] SR: 0.03986 | CLS: 0.40698 | Train: 86.11% | Test: 80.28%\n",
      "Epoch [ 27/50] SR: 0.03997 | CLS: 0.40659 | Train: 86.18% | Test: 80.80%\n",
      "Epoch [ 28/50] SR: 0.04003 | CLS: 0.39642 | Train: 86.41% | Test: 80.88%\n",
      "Epoch [ 29/50] SR: 0.04029 | CLS: 0.39041 | Train: 86.71% | Test: 80.84%\n",
      "Epoch [ 30/50] SR: 0.03986 | CLS: 0.38562 | Train: 86.95% | Test: 80.83%\n",
      "Epoch [ 31/50] SR: 0.04022 | CLS: 0.37956 | Train: 87.08% | Test: 80.81%\n",
      "Epoch [ 32/50] SR: 0.04016 | CLS: 0.37862 | Train: 87.13% | Test: 80.60%\n",
      "Epoch [ 33/50] SR: 0.04086 | CLS: 0.37228 | Train: 87.26% | Test: 81.02%\n",
      "Epoch [ 34/50] SR: 0.04028 | CLS: 0.37013 | Train: 87.34% | Test: 81.01%\n",
      "Epoch [ 35/50] SR: 0.04017 | CLS: 0.36428 | Train: 87.57% | Test: 80.88%\n",
      "Epoch [ 36/50] SR: 0.04021 | CLS: 0.35956 | Train: 87.75% | Test: 81.00%\n",
      "Epoch [ 37/50] SR: 0.04029 | CLS: 0.35843 | Train: 87.86% | Test: 81.16%\n",
      "Epoch [ 38/50] SR: 0.04049 | CLS: 0.35715 | Train: 87.80% | Test: 81.25%\n",
      "Epoch [ 39/50] SR: 0.04038 | CLS: 0.34836 | Train: 88.15% | Test: 80.68%\n",
      "Epoch [ 40/50] SR: 0.04059 | CLS: 0.34916 | Train: 88.08% | Test: 81.16%\n",
      "Epoch [ 41/50] SR: 0.04033 | CLS: 0.34018 | Train: 88.29% | Test: 80.93%\n",
      "Epoch [ 42/50] SR: 0.04035 | CLS: 0.33701 | Train: 88.36% | Test: 81.09%\n",
      "Epoch [ 43/50] SR: 0.04067 | CLS: 0.33637 | Train: 88.52% | Test: 81.10%\n",
      "Epoch [ 44/50] SR: 0.04039 | CLS: 0.32701 | Train: 88.74% | Test: 81.05%\n",
      "Epoch [ 45/50] SR: 0.04047 | CLS: 0.32770 | Train: 88.69% | Test: 81.34%\n",
      "Epoch [ 46/50] SR: 0.04039 | CLS: 0.32073 | Train: 89.15% | Test: 80.86%\n",
      "Epoch [ 47/50] SR: 0.04027 | CLS: 0.32434 | Train: 89.17% | Test: 81.11%\n",
      "Epoch [ 48/50] SR: 0.04058 | CLS: 0.31889 | Train: 89.14% | Test: 81.17%\n",
      "Epoch [ 49/50] SR: 0.04057 | CLS: 0.31550 | Train: 89.15% | Test: 80.76%\n",
      "Epoch [ 50/50] SR: 0.04083 | CLS: 0.31615 | Train: 89.14% | Test: 81.08%\n",
      "saved to results/multi/\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 50 \n",
    "lr = 0.0005\n",
    "momentum = 0.9\n",
    "lambda_sr = 1.0\n",
    "lambda_cls = 1.0\n",
    "\n",
    "train_dataset = CIFAR10Dataset(root_dir='./DS/CIFAR10', train=True)\n",
    "test_dataset  = CIFAR10Dataset(root_dir='./DS/CIFAR10', train=False)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "fuzz = Fuzz(scale_factor=2)\n",
    "\n",
    "inv_mean = torch.tensor(cifar10_mean).view(3,1,1).to(device)\n",
    "inv_std  = torch.tensor(cifar10_std).view(3,1,1).to(device)\n",
    "\n",
    "def denormalize(x):\n",
    "    return x * inv_std + inv_mean\n",
    "\n",
    "sr_criterion  = nn.L1Loss()\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_sr_loss = 0\n",
    "    epoch_cls_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        clean = denormalize(inputs)\n",
    "        blurry = torch.stack([fuzz(img) for img in clean]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sr_out, cls_out = model(blurry)\n",
    "\n",
    "        loss_sr  = sr_criterion(sr_out, clean)\n",
    "        loss_cls = cls_criterion(cls_out, labels)\n",
    "        loss = lambda_sr * loss_sr + lambda_cls * loss_cls\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_sr_loss  += loss_sr.item()\n",
    "        epoch_cls_loss += loss_cls.item()\n",
    "        _, predicted = torch.max(cls_out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_sr  = epoch_sr_loss / len(train_loader)\n",
    "    avg_cls = epoch_cls_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            clean = denormalize(inputs)\n",
    "            blurry = torch.stack([fuzz(img) for img in clean]).to(device)\n",
    "            _, cls_out = model(blurry)\n",
    "            _, predicted = torch.max(cls_out.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    test_acc = 100 * test_correct / test_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1:3d}/{epochs}] SR: {avg_sr:.5f} | CLS: {avg_cls:.5f} | Train: {train_acc:.2f}% | Test: {test_acc:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"{save_dir}/multi_model.pth\")\n",
    "print(f\"保存至 {save_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c833496",
   "metadata": {},
   "source": [
    "## 对比: 模糊直接分类 vs 多任务(SR+分类)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb5cf47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassifyNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m baseline \u001b[38;5;241m=\u001b[39m \u001b[43mClassifyNet\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m baseline\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/task2/model_epoch_100.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      3\u001b[0m baseline\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClassifyNet' is not defined"
     ]
    }
   ],
   "source": [
    "baseline = ClassifyNet().to(device)\n",
    "baseline.load_state_dict(torch.load(\"results/task2/model_epoch_100.pth\", map_location=device))\n",
    "baseline.eval()\n",
    "model.eval()\n",
    "\n",
    "acc_baseline = 0\n",
    "acc_multi = 0\n",
    "acc_clean = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        clean = denormalize(inputs)\n",
    "        blurry = torch.stack([fuzz(img) for img in clean]).to(device)\n",
    "\n",
    "        upscaled = nn.functional.interpolate(blurry, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "        upscaled_norm = normalize(upscaled)\n",
    "        pred_baseline = baseline(upscaled_norm).argmax(dim=1)\n",
    "\n",
    "        _, cls_out = model(blurry)\n",
    "        pred_multi = cls_out.argmax(dim=1)\n",
    "\n",
    "        pred_clean = baseline(inputs).argmax(dim=1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        acc_baseline += (pred_baseline == labels).sum().item()\n",
    "        acc_multi    += (pred_multi == labels).sum().item()\n",
    "        acc_clean    += (pred_clean == labels).sum().item()\n",
    "\n",
    "acc_baseline = 100 * acc_baseline / total\n",
    "acc_multi    = 100 * acc_multi / total\n",
    "acc_clean    = 100 * acc_clean / total\n",
    "\n",
    "print(f\"清晰图直接分类:       {acc_clean:.2f}%\")\n",
    "print(f\"低分辨率图直接分类:  {acc_baseline:.2f}%\")\n",
    "print(f\"低分辨率图 → 超分 → 分类:       {acc_multi:.2f}%\")\n",
    "print(f\"提升: {acc_multi - acc_baseline:+.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
